
# Missouri DOC Policy Copilot: Technical Documentation

## 1. Project Overview

The **Missouri DOC Policy Copilot** is an advanced Retrieval-Augmented Generation (RAG) chatbot designed to provide accurate, context-aware, and conversational answers to questions about the Missouri Department of Corrections (DOC) policy documents. 

Built with Python, Streamlit, and Google's Gemini Pro API, the application leverages a sophisticated RAG pipeline to understand user queries, retrieve relevant information from a specialized knowledge base of policy PDFs, and generate human-like responses grounded in the source material.

**Core Features:**
- **Automated Document Ingestion:** Automatically processes and indexes policy documents from a designated folder.
- **Advanced RAG Pipeline:** Implements query rewriting, semantic chunking, and reranking to enhance retrieval accuracy and response quality.
- **Gemini Pro Integration:** Uses Google's `gemini-1.5-flash` model for generating high-quality embeddings and generating conversational responses.
- **Conversational UI:** A user-friendly interface built with Streamlit allows for interactive querying and displays chat history in a clean, chronological format.
- **Evaluation & Logging:** Captures key metrics for each interaction, including query details, retrieved chunks, and response quality, for continuous improvement.
- **Ready for Deployment:** The application is containerized and configured for seamless deployment on platforms like Streamlit Cloud.

---

## 2. System Architecture

The application is built on a modular architecture that separates concerns, making it maintainable and scalable.

![System Architecture Diagram](https://i.imgur.com/example.png)  <!-- Placeholder for a real diagram -->

**Key Components:**

1.  **Frontend (Streamlit):** The `streamlit_app.py` script creates the web interface, manages user sessions and state, and orchestrates calls to the backend pipeline.
2.  **Backend (RAG Pipeline):** The `main.py` script encapsulates the core RAG logic. It handles everything from loading data to generating the final answer.
3.  **Vector Store (FAISS):** `vector_store.py` manages the FAISS index, which stores vector embeddings of the policy documents for efficient similarity searches.
4.  **Utilities (`utils.py`):** Contains helper functions for loading PDFs, splitting text into semantic chunks, and cleaning text data.
5.  **Configuration (`config.py`):** Centralizes model names, file paths, and other configuration parameters.
6.  **Evaluation (`evaluation.py`):** Manages the logging of interactions to a CSV file for analysis.
7.  **Knowledge Base (`/data/policies`):** The fixed directory containing all the Missouri DOC policy PDF files that form the chatbot's knowledge base.

---

## 3. The RAG Pipeline Explained

The heart of the Policy Copilot is its advanced RAG pipeline. This pipeline ensures that the answers generated by the LLM are not just fluent but also factually grounded in the provided policy documents.

### Step 1: Data Ingestion and Indexing (The "Retrieval" Foundation)

This is a one-time setup process that prepares the knowledge base.

- **Loading Documents:** The pipeline starts by loading all PDF files from the `data/policies/` directory using the `PyMuPDFLoader` in `utils.py`.
- **Semantic Chunking:** Instead of arbitrarily splitting the text, `utils.py` uses a `SemanticChunker` with Gemini embeddings. This method divides the text into chunks based on semantic similarity, keeping related sentences together. This is more effective than fixed-size chunking because it preserves the context within each chunk.
- **Generating Embeddings:** Each text chunk is converted into a numerical vector (an embedding) using the `GoogleGenerativeAIEmbeddings` model (`text-embedding-004`). These embeddings capture the semantic meaning of the text.
- **Creating the FAISS Index:** The text chunks and their corresponding embeddings are stored in a FAISS (Facebook AI Similarity Search) index. FAISS is highly efficient for searching through millions of vectors. The created index is saved to disk (`index/faiss_index`) for quick loading in subsequent sessions.

This entire process is handled by the `add_new_documents` function in `main.py`.

### Step 2: Query Processing and Retrieval (Real-time)

When a user asks a question, the following steps occur in real-time:

**A. Query Rewriting**

The initial user query might be short, ambiguous, or lack context. To improve retrieval, the pipeline first rewrites the query.

- **Function:** `rewrite_query` in `main.py`.
- **Process:** The original query is sent to the Gemini model with a specific prompt asking it to rephrase the question into a more detailed and context-rich version, suitable for a vector database search.
- **Example:**
    - **Original Query:** "What about inmate mail?"
    - **Rewritten Query:** "What are the specific policies and procedures regarding incoming and outgoing mail for inmates, including regulations on content, inspection, and restricted items?"

**B. Vector Search**

The rewritten query is embedded into a vector using the same Gemini model. The pipeline then searches the FAISS index for the text chunks with embeddings that are most similar to the query's embedding, using cosine similarity.

- **Function:** `search_vector_store` in `vector_store.py`.
- **Output:** A list of the top `k` most relevant document chunks.

**C. Semantic Reranking**

A simple vector search might return chunks that are semantically close but not all equally relevant. Reranking adds a layer of refinement.

- **Function:** `rerank_chunks` in `main.py`.
- **Process:** The retrieved chunks and the original query are passed to a `FlashRankRerank` model. This model re-evaluates the relevance of each chunk to the query and reorders them, pushing the most relevant ones to the top. This step is crucial for filtering out noise and focusing the LLM on the best possible context.

### Step 3: Augmented Generation (The "Generation" Step)

This is where the final answer is created.

- **Function:** `get_llm_response` in `main.py`.
- **Process:**
    1.  The top-ranked, most relevant chunks of text are combined into a single block of context.
    2.  A carefully crafted prompt is constructed. This prompt includes the original user question and the retrieved context.
    3.  The prompt instructs the Gemini model (`gemini-1.5-flash`) to generate a conversational, helpful answer **based only on the provided context**. This final instruction is critical for preventing the model from "hallucinating" or using outside knowledge.
- **Prompt Template Example:**
    ```
    You are a friendly and helpful assistant for the Missouri Department of Corrections. Answer the following question based ONLY on the provided context. The answer should be in plain English, warm, and conversational. Do not cite sources or use jargon.

    CONTEXT:
    {retrieved_chunks_text}

    QUESTION:
    {user_question}

    ANSWER:
    ```
- **Output:** The final, grounded, and conversational answer that is displayed to the user.

---

## 4. Codebase Guide

### `streamlit_app.py`
This file creates the user interface.
- **`main()`:** The entry point for the Streamlit app. It sets up the page title, sidebar, and session state.
- **Session State (`st.session_state`):** Used to maintain the chat history and other variables across user interactions.
- **Chat Display Logic:** It iterates through the `st.session_state.messages` list (which stores pairs of user questions and bot answers) and displays them in chronological order.
- **Input Handling:** It captures the user's query from the input box and triggers the RAG pipeline by calling `rag_chain.invoke()`.

```python
# streamlit_app.py: Simplified chat display loop
for message_pair in st.session_state.messages:
    with st.chat_message("user"):
        st.write(message_pair["user"])
    with st.chat_message("assistant"):
        st.write(message_pair["assistant"])
```

### `main.py`
The core logic of the RAG application.
- **`RAGChain` Class:** An organizational class that holds the entire pipeline.
- **`__init__()`:** Initializes the vector store, reranker, and evaluation logger. It calls `add_new_documents()` to ensure the knowledge base is indexed.
- **`add_new_documents()`:** Checks for new PDFs and indexes them. It avoids re-copying files to prevent errors in deployed environments.
- **`invoke()`:** The main method that executes the entire RAG chain for a given query: rewrite -> search -> rerank -> generate response.

```python
# main.py: The invoke method orchestrating the RAG pipeline
def invoke(self, query: str) -> dict:
    self.logger.log_query(query)
    
    rewritten_query = self.rewrite_query(query)
    self.logger.update_log(rewritten_query=rewritten_query)
    
    retrieved_chunks = self.vector_store.search_vector_store(rewritten_query)
    
    reranked_chunks = self.rerank_chunks(query, chunks=[chunk.page_content for chunk in retrieved_chunks])
    
    context = "\n\n---\n\n".join(reranked_chunks)
    self.logger.update_log(final_context=context)
    
    response = self.get_llm_response(query, context)
    self.logger.update_log(final_response=response, status="complete")
    
    return {"query": query, "answer": response}
```

### `vector_store.py`
Manages the FAISS vector store.
- **`VectorStore` Class:** Handles creation, saving, loading, and searching of the FAISS index.
- **`get_retriever()`:** Creates and returns a retriever object from the FAISS index, configured to find the top `k` similar documents.
- **`search_vector_store()`:** A convenience method that takes a query string and returns the retrieved document chunks.

### `utils.py`
Provides data loading and processing utilities.
- **`load_all_pdfs()`:** Scans the data directory and loads all PDF files using `PyMuPDFLoader`.
- **`get_text_chunks_and_metadata()`:** Takes loaded documents and uses the `SemanticChunker` to split them into meaningful pieces.
- **`clean_text()`:** (Optional) A placeholder for any text cleaning operations needed before embedding.

### `evaluation.py`
Handles logging for analysis and debugging.
- **`EvaluationLogger` Class:** Provides methods to log different stages of the RAG pipeline.
- **`log_query()`, `update_log()`, `save_log()`:** Methods to create a new log entry for a query, update it with retrieved context and the final response, and save the complete log row to a CSV file (`evaluation_log.csv`).

---

## 5. Setup and Usage

### Prerequisites
- Python 3.8+
- Git

### Local Setup
1.  **Clone the Repository:**
    ```bash
    git clone <your-repo-url>
    cd DOC_Policy_Copilot
    ```
2.  **Create a Virtual Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Set Up Environment Variables:**
    - Create a file named `.env` in the project root.
    - Add your Gemini API key to it:
      ```
      GEMINI_API_KEY="your_api_key_here"
      ```
5.  **Add Policy Documents:**
    - Place your Missouri DOC policy PDF files into the `data/policies/` directory.

### Running the Application
1.  **Launch the Streamlit App:**
    ```bash
    streamlit run streamlit_app.py
    ```
2.  Open your web browser and navigate to `http://localhost:8501`.
3.  The first time you run it, the app will process and index the PDFs. This may take a few minutes. Subsequent runs will be much faster as they will load the pre-built index.
4.  Start asking questions!

---

## 6. Deployment on Streamlit Cloud

1.  **Push to GitHub:** Ensure your entire project, including the PDF files in `data/policies/`, is committed and pushed to a GitHub repository. Your `.gitignore` should NOT exclude the PDF files.
2.  **Create a Streamlit Cloud Account:** Sign up or log in to [Streamlit Cloud](https://share.streamlit.io/).
3.  **Deploy App:**
    - Click "New app" and connect your GitHub account.
    - Select the repository and the `main` branch.
    - Ensure the "Main file path" is set to `streamlit_app.py`.
4.  **Add Secrets:**
    - In the app's advanced settings, go to the "Secrets" section.
    - Add your Gemini API key as a secret. The key name must be `GEMINI_API_KEY`.
    - **Secret format:**
      ```toml
      GEMINI_API_KEY="your_api_key_here"
      ```
5.  **Deploy:** Click "Deploy!". Streamlit will build and launch your application.

---

## 7. Future Enhancements

- **Hybrid Search:** Combine keyword-based search (like BM25) with the existing vector search to improve retrieval for queries containing specific codes or acronyms.
- **Advanced Agentic Behavior:** Implement a ReAct (Reasoning and Acting) agent that can break down complex, multi-step questions and perform multiple retrievals if necessary.
- **Citation Support:** Modify the generation prompt to include citations (e.g., source document and page number) in the final answer to enhance transparency and trust.
- **Feedback Mechanism:** Add "thumbs up/down" buttons in the UI to collect user feedback on answer quality, which can be used to fine-tune the models.

